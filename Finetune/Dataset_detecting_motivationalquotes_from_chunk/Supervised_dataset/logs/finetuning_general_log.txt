[2025-08-19 16:26:18] 
----------------Loading/initalizing MODEL-------------

[2025-08-19 16:26:30] Model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)
          (qkv_proj): Linear4bit(in_features=3072, out_features=5120, bias=False)
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)
[2025-08-19 16:26:30] clearing cache after model loading...
[2025-08-19 16:26:30] 
----------------Loading tokenizer-------------

[2025-08-19 16:26:31] tokenizer length: 200030
[2025-08-19 16:26:31] tokenizer.pad token was None changed it to : <｜PAD▁TOKEN｜>
[2025-08-19 16:26:31] Vocab length BEFORE training: 200030
[2025-08-19 16:26:31] Tokenizer.pad_token already exist: <｜PAD▁TOKEN｜>
[2025-08-19 16:26:31] clearing cache after tokenizer...
[2025-08-19 16:26:31] clearing cache after prepare model for kbit training...
[2025-08-19 16:26:32] 🟢 Trainable parameters: ['base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight']
[2025-08-19 16:26:32] 
⚪️ Frozen parameters: ['base_model.model.model.embed_tokens.weight', 'base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.0.mlp.gate_up_proj.weight', 'base_model.model.model.layers.0.mlp.down_proj.base_layer.weight']
[2025-08-19 16:26:32] Perft_config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='C:\\Users\\didri\\Desktop\\LLM-models\\LLM-Models\\microsoft\\unsloth\\Phi-4-mini-instruct', revision=None, inference_mode=False, r=32, target_modules={'o_proj', 'qkv_proj', 'down_proj'}, exclude_modules=None, lora_alpha=64, lora_dropout=0.03, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None) 
 
[2025-08-19 16:26:32] Model is prepared for kbit training now...
 model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (qkv_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=5120, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=5120, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=8192, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)

[2025-08-19 16:26:32] model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (qkv_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=5120, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=5120, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=8192, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)

[2025-08-19 16:26:32] 
----------------Dataset Initizalation-------------

[2025-08-19 16:26:32] clearing cache after loading dataset
[2025-08-19 16:27:08] clearing cache after validation test
[2025-08-19 16:27:08] ----------------Starting Training now----------------
[2025-08-19 16:32:15] clearing cache after training complete
[2025-08-19 16:32:16] clearing cache after evaluation complete
[2025-08-19 16:35:12] Successfully done finetuning!
[2025-08-19 16:35:12] --------------Running a manual test--------------------


[2025-08-19 16:35:12] SUCCESS :D
[2025-08-19 16:36:07] 
----------------Loading/initalizing MODEL-------------

[2025-08-19 16:36:16] Model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)
          (qkv_proj): Linear4bit(in_features=3072, out_features=5120, bias=False)
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)
[2025-08-19 16:36:16] clearing cache after model loading...
[2025-08-19 16:36:16] 
----------------Loading tokenizer-------------

[2025-08-19 16:36:17] tokenizer length: 200030
[2025-08-19 16:36:17] tokenizer.pad token was None changed it to : <｜PAD▁TOKEN｜>
[2025-08-19 16:36:17] Vocab length BEFORE training: 200030
[2025-08-19 16:36:17] Tokenizer.pad_token already exist: <｜PAD▁TOKEN｜>
[2025-08-19 16:36:17] clearing cache after tokenizer...
[2025-08-19 16:36:17] clearing cache after prepare model for kbit training...
[2025-08-19 16:36:17] 🟢 Trainable parameters: ['base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight']
[2025-08-19 16:36:17] 
⚪️ Frozen parameters: ['base_model.model.model.embed_tokens.weight', 'base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.0.mlp.gate_up_proj.weight', 'base_model.model.model.layers.0.mlp.down_proj.base_layer.weight']
[2025-08-19 16:36:17] Perft_config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='C:\\Users\\didri\\Desktop\\LLM-models\\LLM-Models\\microsoft\\unsloth\\Phi-4-mini-instruct', revision=None, inference_mode=False, r=32, target_modules={'o_proj', 'qkv_proj', 'down_proj'}, exclude_modules=None, lora_alpha=64, lora_dropout=0.03, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None) 
 
[2025-08-19 16:36:17] Model is prepared for kbit training now...
 model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (qkv_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=5120, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=5120, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=8192, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)

[2025-08-19 16:36:17] model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (qkv_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=5120, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=5120, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=8192, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)

[2025-08-19 16:36:17] 
----------------Dataset Initizalation-------------

[2025-08-19 16:36:18] clearing cache after loading dataset
[2025-08-19 16:36:59] clearing cache after validation test
[2025-08-19 16:36:59] ----------------Starting Training now----------------
[2025-08-19 19:30:03] clearing cache after training complete
[2025-08-19 19:30:05] clearing cache after evaluation complete
[2025-08-19 21:47:09] 
----------------Loading/initalizing MODEL-------------

[2025-08-19 21:47:13] Model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)
          (qkv_proj): Linear4bit(in_features=3072, out_features=5120, bias=False)
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)
[2025-08-19 21:47:13] clearing cache after model loading...
[2025-08-19 21:47:14] 
----------------Loading tokenizer-------------

[2025-08-19 21:47:14] tokenizer length: 200030
[2025-08-19 21:47:14] tokenizer.pad token was None changed it to : <｜PAD▁TOKEN｜>
[2025-08-19 21:47:14] Vocab length BEFORE training: 200030
[2025-08-19 21:47:14] Tokenizer.pad_token already exist: <｜PAD▁TOKEN｜>
[2025-08-19 21:47:14] clearing cache after tokenizer...
[2025-08-19 21:47:14] clearing cache after prepare model for kbit training...
[2025-08-19 21:47:15] 🟢 Trainable parameters: ['base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight']
[2025-08-19 21:47:15] 
⚪️ Frozen parameters: ['base_model.model.model.embed_tokens.weight', 'base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.0.mlp.gate_up_proj.weight', 'base_model.model.model.layers.0.mlp.down_proj.base_layer.weight']
[2025-08-19 21:47:15] Perft_config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='C:\\Users\\didri\\Desktop\\LLM-models\\LLM-Models\\microsoft\\unsloth\\test', revision=None, inference_mode=False, r=32, target_modules={'down_proj', 'qkv_proj', 'o_proj'}, exclude_modules=None, lora_alpha=64, lora_dropout=0.03, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None) 
 
[2025-08-19 21:47:15] Model is prepared for kbit training now...
 model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (qkv_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=5120, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=5120, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=8192, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)

[2025-08-19 21:47:15] model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (qkv_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=5120, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=5120, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=8192, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)

[2025-08-19 21:47:15] 
----------------Dataset Initizalation-------------

[2025-08-19 21:47:15] clearing cache after loading dataset
[2025-08-19 21:50:15] 
----------------Loading/initalizing MODEL-------------

[2025-08-19 21:50:20] Model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)
          (qkv_proj): Linear4bit(in_features=3072, out_features=5120, bias=False)
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)
[2025-08-19 21:50:20] clearing cache after model loading...
[2025-08-19 21:50:20] 
----------------Loading tokenizer-------------

[2025-08-19 21:50:21] tokenizer length: 200030
[2025-08-19 21:50:21] tokenizer.pad token was None changed it to : <｜PAD▁TOKEN｜>
[2025-08-19 21:50:21] Vocab length BEFORE training: 200030
[2025-08-19 21:50:21] Tokenizer.pad_token already exist: <｜PAD▁TOKEN｜>
[2025-08-19 21:50:21] clearing cache after tokenizer...
[2025-08-19 21:50:21] clearing cache after prepare model for kbit training...
[2025-08-19 21:50:22] 🟢 Trainable parameters: ['base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight']
[2025-08-19 21:50:22] 
⚪️ Frozen parameters: ['base_model.model.model.embed_tokens.weight', 'base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.0.mlp.gate_up_proj.weight', 'base_model.model.model.layers.0.mlp.down_proj.base_layer.weight']
[2025-08-19 21:50:22] Perft_config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='C:\\Users\\didri\\Desktop\\LLM-models\\LLM-Models\\microsoft\\unsloth\\test', revision=None, inference_mode=False, r=32, target_modules={'qkv_proj', 'o_proj', 'down_proj'}, exclude_modules=None, lora_alpha=64, lora_dropout=0.03, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None) 
 
[2025-08-19 21:50:22] Model is prepared for kbit training now...
 model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (qkv_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=5120, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=5120, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=8192, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)

[2025-08-19 21:50:22] model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (qkv_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=5120, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=5120, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=8192, out_features=32, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=32, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)

[2025-08-19 21:50:22] 
----------------Dataset Initizalation-------------

[2025-08-19 21:50:22] clearing cache after loading dataset
[2025-08-20 18:29:31] 
----------------Loading/initializing MODEL-------------

[2025-08-20 18:29:36] Model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)
          (qkv_proj): Linear4bit(in_features=3072, out_features=5120, bias=False)
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)
[2025-08-20 18:29:37] 
----------------Dataset Initialization-------------

[2025-08-20 18:29:38] Tokenizer length: 200030
[2025-08-20 18:29:38] Tokenizer.pad_token: <｜PAD▁TOKEN｜>
[2025-08-20 18:29:38] Vocab length BEFORE training: 200030
[2025-08-20 18:29:38] FULL TEXT: <|system|>You are an attentive assistant who reads everything carefully. Your primary goal is to identify self-contained text for creating motivational shorts by identifying short-form motivational or self-improvement statements and anecdotes that inspire personal growth, resilience, and self-reflection — often using contrasts, memorable insights, or real-life stories to encourage positive mindset, discipline, and perseverance. Your task is to carefully analyze and read a timestamped chunk and extract any Qualifying motivational texts, inspirational passages, or self-contained statements that offer encouragement, life advice, or inspiration and could be used for a 15 - 20 seconds short video. If such texts are found, save them using the appropriate function. If none are found, return a final response indicating that.
You are to save standalone Qualifying motivational texts, advice, inspiring messages, or passages that are complete and do not lack context or would confuse a listener if isolated from the rest of the chunk.
Analyze the chunk/transcript between [chunk start] and [chunk end].
Objective: Your job is to extract motivational qualifying texts from the input text chunk/transcript. 
These are typically short, self-contained passages that offer encouragement, life advice, or inspiration — including inspiring messages, anecdotes, or insights (not limited to direct quotes).
Reasoning: Always begin with a Thought: statement explaining your reasoning — for example, summarize the intent of the chunk, indicate whether you identified any motivational texts (and how many), or explain why none qualified.
ABSOLUTE OUTPUT FORMAT RULE
YOU must output exactly:
    Thought: [your reasoning text here]  
    <code>  
    [one or more SaveMotivationalText(...) calls if any found]  
    [exactly one final_answer(...) call]  
    </code>

Your output format rules:
    1. Thought: must always be outside <code>.
    2. <code> must contain nothing except function calls.
    3. Always provide a 'Thought:' sequence, and a '<code>' sequence ending with '</code>', else you will fail.
    4. The Thought: sequence must always provide a short reasoning over the overall intent of the chunk, describing what the speaker/text is mainly doing (e.g., telling a story, joking, giving advice, reflecting, describing an event). Then, based on that intent, explain whether any qualifying motivational texts were found or not. If qualifying texts were found, explain briefly why those specific passages qualify (mention their motivational themes like effort, discipline, resilience, growth, mindset shift, etc.), why other parts of the chunk were excluded (e.g., anecdotal, casual, off-topic), and explicitly confirm that the saved motivational texts are complete, self-contained, and do not lack context when isolated. If no qualifying texts were found, explain that the chunk’s intent (story, casual chat, filler, incomplete thought, etc.) did not contain any standalone motivational passages suitable for a short video.

After analyzing chunk:
    -If one or more qualifying motivational texts are found: Output one SaveMotivationalText(...) call for each self-contained qualifying text.
    -After the last SaveMotivationalText(...) call, output exactly one final_answer("im done").
    -If none qualifying motivational texts  are found: 
        -Output only one line: final_answer("After carefully analysing the chunk/text, i have concluded nothing can be saved. Nothing qualifies for a motivational shorts video, No text had a clear beginning & end or a profound overall intent/meaning that is suitable for a motivational shorts. Nothing that would provide any interest for a listener.")

Here are 3 examples of correct output:
Exsample 1.  If two qualifying motivational texts are found in the chunk you analyze, You must output:
Thought: [Your reasoning here...]
<code>
    SaveMotivationalText(text="[623.70s - 627.11s] The magic you are looking for [627.11s - 640.14s]  is in the work you are avoiding", text_file=text_file)  
    SaveMotivationalText(text="[500.00s - 502.34s] You don't need perfect conditions to make progress. [502.34s - 505.22s] You just need to move", text_file=text_file)    
    final_answer("im done")
</code>

Exsample 2.  If one qualifying motivational text is found in the chunk/transcript that you analyze, You must output:
Thought: [your reasoning here...]
<code>
    SaveMotivationalText(text="[617.70s - 627.11s] You will encounter many challenges in life  [627.12s - 628.00s] But you must never be defeated by [628.01s - 629.55s] the challenges", text_file=text_file)
    final_answer("im done")
</code>    

Exsample 3. If no qualifying motivational texts are found in the chunk you analyze, You should output the reason in the The 'Thought' sequence, a short reason of the intent and noting that that the chunk (inferred as non-motivational or lacks a clear intent):
Thought: [your reasoning here...]
<code>
    final_answer("After carefully analysing the chunk/text, i have concluded nothing can be saved. Nothing qualifies for a motivational shorts video, That would grab the attention of a listener")
</code>

Timestamp Handling when saving qualifying text:
- When a motivational text spans multiple lines (each line containing a separate timestamp):
- Merge the lines into a single text.
- Include the start time from the first line and all timestamps up to and including the end time from the last line, Example: SaveMotivationalText(text="[617.70s - 627.11s] You will encounter many challenges in life [627.12s - 628.00s] But you must never be defeated by [628.01s - 629.55s] the challenges", text_file=text_file).
- Preserve original spacing and punctuation exactly.
- Example output: SaveMotivationalText("[start - end] Qualifying text line 1 [start - end] Qualifying text line 2 [start - end]", text_file=text_file) if the qualifying text spans multiple lines.
- The timestamps in the format [SSSS.SSs - SSSS.SSs] represent the time in seconds for when the words are spoken in the video transcript.

Here you have Important Rules/instructions to follow:
- Motivational texts must be inspirational and standalone — avoid fragments or generic sentences.
- Always include both `Thought:` and `<code>` blocks.
- Use exact function names and punctuation as shown.
- Do not return texts that are incomplete or unclear.
- Do not create multiple SaveMotivationalText() calls for each line in a single text.
- Do not alter or guess missing timestamps — use the exact start and end values provided in the lines that contain the text.
- Text should appear as a single, continuous string, even if it was originally split across 1-8 lines.   
- Each line in a chunk has a timestamp ([start - end]) that represent the time of those words spoken, the transcript you are analyzing is text from a video transcribed from a audio.
- The chunks you analyze may vary in size. Always analyze the entire chunk and identify any qualifying text if any before providing <code>.

Types of Qualifying Motivational Texts:
- Passages that encourage perseverance, personal growth, resilience, mindset shift, success, discipline, consistency, or overcoming challenges (e.g., "You will encounter many challenges in life, but you must never be defeated by them"). A complete text that does not lack context when isolated.
- Action-oriented advice that inspires immediate steps toward improvement (e.g., "You don’t need perfect conditions to make progress. You just need to move").
- Messages promoting self-belief, confidence, or personal growth (e.g., "The difference between who you are and who you want to be is in the choices you make every day").
- Universal life advice that is concise and impactful (e.g., "Discipline is choosing what you want most over what you want now").
- Inspirational statements that evoke hope or determination (e.g., "Fear is loud, but your future deserves to be louder").
- Anecdotes or stories that illustrate growth, resilience, or lessons (e.g., short, self-contained real-life examples from temp.txt-like files).

Notes on Qualifying Texts:
- Texts must be self-contained, meaning they convey a complete thought without needing surrounding context.
- Avoid generic statements (e.g., "Life is hard" or "mindset is good") or fragments that lack clear motivational intent and would not grab a listener's attention.
- Ensure the passage is concise enough for a motivational shorts video.
- Do not save text that does not form a complete thought. If it lacks context when isolated from the rest of the transcript, do not save it. You must understand the overall meaning of the text to judge whether it stands on its own. If it's not a self-contained motivational passage, you will fail.

Here are 5 few-shot examples of qualifying texts, and If you identify similar texts like these that qualifies in a transcript or chunk that you analyze, save it with SaveMotivationalText("...",text_file=text_file) as they are self-contained and suitable for  motivational shorts video:
    -----------------
    1. "always keep going there's been times particularly early in my career where it just feels like thisis the end but what i've come to find out is that no matter what happens the stormeventually ends and when the storm does end you want to make sure that you're ready"
        Reason: Qualifies because it’s a self-contained motivational lesson about perseverance and resilience. It conveys a complete thought that encourages hope and preparation.


    2. "James Clear has this fucking unbelievable insight. It doesn't make sense to continue wanting something if you're not willing to do what it takes to get it. If you don't want to live the lifestyle, then release yourself from the desire. To crave the result, but not the process, is to guarantee disappointment"
        Reason: Qualifies because it offers a profound motivational insight with complete context. It delivers actionable advice on aligning desires with actions, making it suitable as a standalone short.
    
    3. "The magic you are looking for is in the work you are avoiding"
         Reason: Qualifies because it’s concise, memorable, and self-contained. It provides clear motivational advice about discipline and effort without requiring extra context.

    4. "Willpower is the key to success. Successful people strive no matter what they feel by applying their will to overcome apathy, doubt or fear."
        Reason:  Qualifies because it presents a universal motivational principle about resilience and determination. It is self-contained and inspires immediate application.
    
    5. "Discipline isn't just about following rigid rules or punishing yourself for slip-ups; it's the bridge between your dreams and reality, where every small, consistent action you take today—like waking up early to work on your goals despite feeling tired—compounds into massive personal growth tomorrow, turning potential into achievement and weakness into unshakeable strength"
        Reason: Qualifies because it’s a full motivational passage that explains discipline in a clear, actionable, and inspirational way. It has a strong beginning, middle, and end, making it suitable as a complete short.
    
Here are 5 few-shot examples of Texts that does not Qualify and that you must never save with SaveMotivationalText("...",text_file=text_file) Even if they appear motivational at first glance, they must be excluded. Below the texts is a (reason) that explain why the text does not qualify. Thought: must reflect why they fail to qualify: these texts are either incomplete, generic, dependent on prior context, or too vague to stand on their own as a motivational short. Saving them would confuse or disengage a listener because they lack a clear beginning, end, or a profound intent.
    -----------------
    1. "Life can be tough sometimes" 
      Reason: excluded because Not qualifying text because Too generic, lacks insight or actionable advice, not engaging enough for a short video.

    2. "And that's why you should keep trying" 
        Reason: excluded because Depends on missing context (what was discussed before), incomplete if isolated.

    3. "Like we discussed before, success is important" 
        Reason: excluded because References prior discussion, cannot stand alone, vague and redundant.

    4. "I got to push you back. I got to stop you from going that hard." 
        Reason: excluded because Fragmented statement, lacks motivational theme or universal meaning, unclear intent.
        
    5. "But I said, at this point, I think I've been working out like crazy person" 
        Reason: excluded because Personal anecdote without a motivational lesson, lacks self-contained insight.<|end|><|user|>Your task is to Identify Qualifying Motivational Texts & Save them if any is found in the chunk.
        Here is the chunk you must analyze:
[chunk start]
[716.08s - 719.79s] Cause she's doing it very pretty girl.
[719.89s - 724.30s] You are not defined by your
[724.40s - 728.13s] past mistakes; they are merely lessons
[728.23s - 732.75s] that prepare you for
[732.85s - 736.29s] greater victories Forgive yourself, rise
[736.39s - 739.95s] above the regrets, and channel that
[740.05s - 744.79s] energy into creating a future filled
[744.89s - 748.95s] with purpose and achievement
[749.05s - 752.88s] but I can realize in, in, in saying it now, like I'm quite moved by the fact that they would ask that
[752.98s - 756.29s] Here's how to get older without getting better. Keep relearning the same lesson.
[756.39s - 761.07s] You are not defined
[761.17s - 765.64s] by your circumstances; you
[765.74s - 769.17s] define them Take control,
[769.27s - 773.42s] make bold moves, and
[773.52s - 778.01s] craft a story of
[778.11s - 782.63s] triumph and resilience
[782.73s - 786.48s] flavors with your first box by going to the link in the description below or heading to drinklmnt.com
[786.58s - 790.39s] And I have the spots picked out and I'm already working all of the hours that I'm awake.
[790.49s - 793.88s] And I, no, don't get me wrong.
[chunk end]<|end|><|assistant|>Thought: The text intersperses casual remarks and a brief insight on learning lessons with two direct motivational statements about overcoming past mistakes and circumstances, amid promotional and personal notes. Based on that intent, two qualifying motivational texts were found, which qualify due to their themes of personal growth, resilience, forgiveness, mindset shift, and triumph over challenges; other parts were excluded as they are generic, promotional, or anecdotal without inspirational value; the saved motivational texts are complete, self-contained, and do not lack context when isolated. 
<code>
SaveMotivationalText(text="[719.89s - 724.30s] You are not defined by your [724.40s - 728.13s] past mistakes; they are merely lessons [728.23s - 732.75s] that prepare you for [732.85s - 736.29s] greater victories Forgive yourself, rise [736.39s - 739.95s] above the regrets, and channel that [740.05s - 744.79s] energy into creating a future filled [744.89s - 748.95s] with purpose and achievement", text_file=text_file)
SaveMotivationalText(text="[756.39s - 761.07s] You are not defined [761.17s - 765.64s] by your circumstances; you [765.74s - 769.17s] define them Take control, [769.27s - 773.42s] make bold moves, and [773.52s - 778.01s] craft a story of [778.11s - 782.63s] triumph and resilience", text_file=text_file)final_answer("im done")
</code><|end|><|end|>
[2025-08-20 18:29:38] INPUT_IDS: [200022, 3575, 553, 448, 79718, 29186, 1218, 31523, 5519, 18455, 13, 4886, 9107, 8583, 382, 316, 11410, 1051, 170373, 2201, 395, 9278, 121706, 51091, 656, 37144, 4022, 16674, 121706, 503, 1051, 42231, 188032, 20673, 326, 149561, 484, 34326, 3832, 8266, 11, 70355, 11, 326, 1051, 62130, 1659, 2733, 4783, 2360, 144049, 11, 38586, 26890, 11, 503, 1374, 40384, 9970, 316, 17636, 8841, 53814, 11, 35389, 11, 326, 136719, 13, 4886, 5296, 382, 316, 18455, 30532, 326, 1729, 261, 24753, 295, 23958, 326, 13424, 1062, 11452, 13700, 121706, 35562, 11, 74723, 69047, 11, 503, 1051, 170373, 20673, 484, 2961, 65308, 11, 2615, 10519, 11, 503, 21907, 326, 2023, 413, 2061, 395, 261, 220, 1055, 533, 220, 455, 12068, 4022, 3823, 13, 1843, 2238, 35562, 553, 2491, 11, 5093, 1373, 2360, 290, 11081, 1114, 13, 1843, 12698, 553, 2491, 11, 622, 261, 1721, 3322, 34700, 484, 558, 3575, 553, 316, 5093, 77670, 11452, 13700, 121706, 35562, 11, 10519, 11, 40477, 10854, 11, 503, 69047, 484, 553, 5533, 326, 621, 625, 11728, 3814, 503, 1481, 108715, 261, 19715, 538, 42329, 591, 290, 2867, 328, 290, 23958, 558, 107202, 290, 23958, 148427, 2113, 2870, 723, 56156, 1604, 60, 326, 723, 56156, 1268, 49887, 82888, 25, 4886, 3349, 382, 316, 13424, 121706, 51855, 35562, 591, 290, 3422, 2201, 23958, 148427, 2113, 13, 793, 13145, 553, 13729, 4022, 11, 1051, 170373, 69047, 484, 2961, 65308, 11, 2615, 10519, 11, 503, 21907, 2733, 3463, 40477, 10854, 11, 149561, 11, 503, 26890, 350, 2878, 9913, 316, 2823, 22661, 6294, 30377, 289, 25, 30141, 4252, 483, 261, 52191, 25, 9710, 45379, 634, 57927, 2733, 395, 4994, 11, 104833, 290, 9841, 328, 290, 23958, 11, 21611, 5588, 481, 17800, 1062, 121706, 35562, 350, 427, 1495, 1991, 936, 503, 16644, 4436, 12698, 18768, 558, 83673, 3162, 21914, 73864, 123805, 83420, 198, 90812, 2804, 4733, 9707, 734, 271, 52191, 25, 723, 35156, 57927, 2201, 2105, 60, 4066, 271, 464, 3056, 29, 4066, 271, 723, 690, 503, 945, 15564, 56197, 349, 1953, 1279, 96375, 11666, 538, 1062, 2491, 60, 4066, 271, 723, 86898, 423, 1001, 1721, 65702, 96375, 2421, 60, 4066, 271, 1040, 3056, 3037, 9719, 4733, 6011, 9607, 734, 271, 220, 16, 13, 52191, 25, 2804, 3324, 413, 7539, 464, 3056, 58796, 271, 220, 17, 13, 464, 3056, 29, 2804, 10232, 6939, 6384, 1114, 11666, 558, 271, 220, 18, 13, 30141, 3587, 261, 461, 108118, 9311, 16281, 11, 326, 261, 8144, 3056, 12747, 16281, 22013, 483, 17833, 3056, 46531, 1203, 481, 738, 7111, 558, 271, 220, 19, 13, 623, 52191, 25, 16281, 2804, 3324, 3587, 261, 4022, 57927, 1072, 290, 10705, 9841, 328, 290, 23958, 11, 42904, 1412, 290, 26150, 62114, 382, 19294, 5306, 350, 68, 1940, 4213, 20186, 261, 4869, 11, 126672, 11, 9874, 10519, 11, 66890, 11, 42904, 448, 2104, 741, 7801, 11, 4122, 402, 484, 9841, 11, 16644, 5588, 1062, 51855, 121706, 35562, 1504, 2491, 503, 625, 13, 1843, 51855, 35562, 1504, 2491, 11, 16644, 51088, 4436, 2617, 4857, 69047, 32123, 350, 112216, 1043, 121706, 28085, 1299, 6942, 11, 35389, 11, 70355, 11, 8266, 11, 53814, 12847, 11, 5178, 36196, 4436, 1273, 7881, 328, 290, 23958, 1504, 52015, 350, 68, 1940, 4213, 84573, 3556, 11, 21860, 11, 1277, 136770, 936, 326, 45615, 7620, 484, 290, 10576, 121706, 35562, 553, 5533, 11, 1051, 170373, 11, 326, 621, 625, 11728, 3814, 1261, 42329, 13, 1843, 860, 51855, 35562, 1504, 2491, 11, 16644, 484, 290, 23958, 802, 9841, 350, 50482, 11, 21860, 7999, 11, 83267, 11, 60592, 4525, 11, 5178, 8476, 2242, 625, 10232, 1062, 77670, 121706, 69047, 13127, 395, 261, 4022, 3823, 364, 8188, 57465, 23958, 734, 271, 533, 3335, 1001, 503, 945, 51855, 121706, 35562, 553, 2491, 25, 18315, 1001, 15564, 56197, 349, 1953, 1279, 96375, 2421, 395, 2454, 1051, 170373, 51855, 2201, 558, 271, 533, 8188, 290, 2174, 15564, 56197, 349, 1953, 1279, 96375, 2421, 11, 4733, 9707, 1001, 1721, 65702, 568, 321, 4167, 88948, 271, 533, 3335, 12698, 51855, 121706, 35562, 220, 553, 2491, 25, 793, 309, 533, 6310, 1606, 1001, 2543, 25, 1721, 65702, 568, 8188, 18455, 169239, 290, 23958, 62114, 11, 575, 679, 41549, 6939, 665, 413, 10576, 13, 23207, 136198, 395, 261, 121706, 51091, 3823, 11, 3004, 2201, 1458, 261, 5364, 10526, 827, 1268, 503, 261, 45307, 10705, 9841, 14, 91529, 484, 382, 13127, 395, 261, 121706, 51091, 13, 23207, 484, 1481, 3587, 1062, 3425, 395, 261, 19715, 79197, 12253, 553, 220, 18, 15652, 328, 6145, 4733, 734, 992, 28434, 220, 16, 13, 220, 1843, 1920, 51855, 121706, 35562, 553, 2491, 306, 290, 23958, 481, 30532, 11, 1608, 2804, 4733, 734, 108118, 25, 723, 9719, 57927, 2105, 1008, 1592, 27, 3056, 523, 271, 15564, 56197, 349, 1953, 1279, 13414, 184023, 43239, 13, 2789, 82, 533, 220, 44364, 13, 994, 82, 60, 623, 19745, 481, 553, 3778, 395, 723, 44364, 13, 994, 82, 533, 220, 25479, 13, 1265, 82, 60, 220, 382, 306, 290, 1101, 481, 553, 49354, 672, 2201, 5933, 115157, 5933, 8, 4066, 271, 15564, 56197, 349, 1953, 1279, 13414, 184023, 3234, 13, 504, 82, 533, 220, 32451, 13, 3020, 82, 60, 1608, 4128, 1309, 4387, 6409, 316, 1520, 7408, 13, 723, 32451, 13, 3020, 82, 533, 220, 31659, 13, 1709, 82, 60, 1608, 1327, 1309, 316, 5275, 672, 2201, 5933, 115157, 5933, 8, 1944, 271, 1721, 65702, 568, 321, 4167, 1896, 808, 3056, 3037, 992, 28434, 220, 17, 13, 220, 1843, 1001, 51855, 121706, 2201, 382, 2491, 306, 290, 23958, 148427, 2113, 484, 481, 30532, 11, 1608, 2804, 4733, 734, 108118, 25, 723, 35156, 57927, 2105, 1008, 1592, 27, 3056, 523, 271, 15564, 56197, 349, 1953, 1279, 13414, 184023, 40657, 13, 2789, 82, 533, 220, 44364, 13, 994, 82, 60, 1608, 738, 19673, 1991, 13525, 306, 2615, 220, 723, 44364, 13, 899, 82, 533, 220, 44008, 13, 504, 82, 60, 3072, 481, 2804, 3779, 413, 53738, 656, 723, 44008, 13, 2290, 82, 533, 220, 46156, 13, 3152, 82, 60, 290, 13525, 672, 2201, 5933, 115157, 5933, 446, 271, 1721, 65702, 568, 321, 4167, 1896, 808, 3056, 29, 24372, 992, 28434, 220, 18, 13, 1843, 860, 51855, 121706, 35562, 553, 2491, 306, 290, 23958, 481, 30532, 11, 1608, 1757, 4733, 290, 5207, 306, 290, 623, 461, 108118, 6, 16281, 11, 261, 4022, 5207, 328, 290, 9841, 326, 60394, 484, 484, 290, 23958, 350, 258, 14366, 472, 2893, 2126, 117352, 1953, 503, 71357, 261, 5364, 9841, 1883, 108118, 25, 723, 35156, 57927, 2105, 1008, 1592, 27, 3056, 523, 271, 1721, 65702, 568, 8188, 18455, 169239, 290, 23958, 62114, 11, 575, 679, 41549, 6939, 665, 413, 10576, 13, 23207, 136198, 395, 261, 121706, 51091, 3823, 11, 4569, 1481, 18013, 290, 8684, 328, 261, 19715, 1896, 808, 3056, 3037, 26838, 93478, 1261, 20519, 51855, 2201, 734, 12, 4296, 261, 121706, 2201, 78545, 7598, 8698, 350, 21309, 2543, 15683, 261, 13574, 24753, 1883, 12, 71009, 290, 8698, 1511, 261, 4590, 2201, 558, 12, 50210, 290, 1604, 1058, 591, 290, 1577, 2543, 326, 722, 104931, 869, 316, 326, 3463, 290, 1268, 1058, 591, 290, 2174, 2543, 11, 24855, 25, 15564, 56197, 349, 1953, 1279, 13414, 184023, 40657, 13, 2789, 82, 533, 220, 44364, 13, 994, 82, 60, 1608, 738, 19673, 1991, 13525, 306, 2615, 723, 44364, 13, 899, 82, 533, 220, 44008, 13, 504, 82, 60, 3072, 481, 2804, 3779, 413, 53738, 656, 723, 44008, 13, 2290, 82, 533, 220, 46156, 13, 3152, 82, 60, 290, 13525, 672, 2201, 5933, 115157, 5933, 6294, 12, 140469, 4756, 47236, 326, 107300, 9707, 558, 12, 24855, 4733, 25, 15564, 56197, 349, 1953, 1279, 21812, 5236, 533, 1268, 60, 11452, 13700, 2201, 2543, 220, 16, 723, 5236, 533, 1268, 60, 11452, 13700, 2201, 2543, 220, 17, 723, 5236, 533, 1268, 41455, 2201, 5933, 115157, 5933, 8, 538, 290, 51855, 2201, 78545, 7598, 8698, 558, 12, 623, 104931, 306, 290, 6011, 723, 3642, 3642, 1242, 146129, 533, 336, 3642, 50, 1242, 146129, 60, 5188, 290, 1058, 306, 12068, 395, 1261, 290, 6391, 553, 36116, 306, 290, 3823, 63989, 364, 12253, 481, 679, 63568, 37317, 26985, 11376, 316, 2622, 734, 12, 99117, 1953, 35562, 2804, 413, 74723, 326, 77670, 2733, 8397, 63899, 503, 21081, 40536, 558, 12, 30141, 3931, 2973, 2700, 108118, 42412, 326, 55472, 3056, 88780, 18125, 558, 12, 7649, 6354, 1114, 8747, 326, 107300, 472, 10147, 558, 12, 3756, 625, 622, 35562, 484, 553, 60592, 503, 59698, 558, 12, 3756, 625, 2501, 7598, 15564, 56197, 349, 1953, 1279, 416, 11666, 395, 2454, 2543, 306, 261, 4590, 2201, 558, 12, 3756, 625, 14183, 503, 11915, 12486, 104931, 2733, 1199, 290, 6354, 1604, 326, 1268, 4824, 5181, 306, 290, 8698, 484, 10232, 290, 2201, 558, 12, 4564, 1757, 7680, 472, 261, 4590, 11, 27979, 1621, 11, 1952, 538, 480, 673, 22257, 12648, 5251, 220, 16, 12, 23, 8698, 13, 10190, 12, 11555, 2543, 306, 261, 23958, 853, 261, 24753, 31338, 5236, 533, 1268, 4636, 484, 5188, 290, 1058, 328, 2617, 6391, 36116, 11, 290, 63989, 481, 553, 57465, 382, 2201, 591, 261, 3823, 1643, 28426, 591, 261, 11065, 558, 12, 623, 53440, 481, 30532, 1340, 14899, 306, 2349, 13, 30141, 30532, 290, 6508, 23958, 326, 11410, 1062, 51855, 2201, 538, 1062, 2254, 9072, 464, 3056, 29, 364, 6125, 328, 11452, 13700, 99117, 1953, 4564, 82, 734, 12, 12984, 1461, 484, 17636, 136719, 11, 3832, 8266, 11, 70355, 11, 53814, 12847, 11, 3505, 11, 35389, 11, 45250, 11, 503, 114793, 13525, 350, 68, 1940, 4213, 392, 3575, 738, 19673, 1991, 13525, 306, 2615, 11, 889, 481, 2804, 3779, 413, 53738, 656, 1373, 3172, 355, 5533, 2201, 484, 2226, 625, 11728, 3814, 1261, 42329, 558, 12, 9905, 49951, 10519, 484, 92977, 22158, 10331, 15256, 20692, 350, 68, 1940, 4213, 392, 3575, 1700, 1573, 1309, 4387, 6409, 316, 1520, 7408, 13, 1608, 1327, 1309, 316, 5275, 88948, 12, 41646, 29569, 1051, 153302, 3156, 11, 16135, 11, 503, 3832, 8266, 350, 68, 1940, 4213, 392, 976, 9809, 2870, 1218, 481, 553, 326, 1218, 481, 1682, 316, 413, 382, 306, 290, 14494, 481, 1520, 1753, 2163, 88948, 12, 35500, 2615, 10519, 484, 382, 82463, 326, 140330, 350, 68, 1940, 4213, 392, 3538, 113811, 382, 20296, 1412, 481, 1682, 1645, 1072, 1412, 481, 1682, 1954, 88948, 12, 178381, 20673, 484, 155045, 5498, 503, 39984, 350, 68, 1940, 4213, 392, 160204, 382, 27900, 11, 889, 634, 5277, 48130, 316, 413, 125931, 88948, 12, 146942, 8301, 8534, 503, 9970, 484, 65836, 8266, 11, 70355, 11, 503, 20636, 350, 68, 1940, 4213, 4022, 11, 1051, 170373, 1374, 40384, 15652, 591, 5484, 7186, 22085, 6291, 3991, 27436, 402, 11452, 13700, 4564, 82, 734, 12, 4564, 82, 2804, 413, 1051, 170373, 11, 10915, 1023, 27769, 261, 5533, 4525, 2935, 44593, 20708, 3814, 558, 12, 46613, 21081, 20673, 350, 68, 1940, 4213, 392, 27182, 382, 3479, 1, 503, 392, 77021, 977, 382, 1899, 1405, 503, 63899, 484, 11728, 5364, 121706, 9841, 326, 1481, 625, 18013, 261, 19715, 885, 8684, 558, 12, 43810, 290, 26368, 382, 82463, 4951, 395, 261, 121706, 51091, 3823, 558, 12, 3756, 625, 5093, 2201, 484, 2226, 625, 1625, 261, 5533, 4525, 13, 1843, 480, 71357, 3814, 1261, 42329, 591, 290, 2867, 328, 290, 63989, 11, 621, 625, 5093, 480, 13, 1608, 2804, 4218, 290, 10705, 10915, 328, 290, 2201, 316, 23319, 5588, 480, 20862, 402, 1617, 2316, 13, 1843, 4275, 625, 261, 1051, 170373, 121706, 26368, 11, 481, 738, 7111, 364, 12253, 553, 220, 20, 3120, 134528, 15652, 328, 51855, 35562, 11, 326, 1843, 481, 11410, 6771, 35562, 1299, 1879, 484, 136198, 306, 261, 63989, 503, 23958, 484, 481, 30532, 11, 5093, 480, 483, 15564, 56197, 349, 1953, 1279, 568, 98809, 919, 5933, 115157, 5933, 8, 472, 1023, 553, 1051, 170373, 326, 13127, 395, 220, 121706, 51091, 3823, 734, 271, 30885, 7413, 271, 220, 16, 13, 392, 55999, 3357, 2966, 15333, 1339, 4238, 11884, 6047, 306, 922, 8461, 1919, 480, 1327, 16591, 1299, 495, 276, 290, 1268, 889, 1412, 103973, 3063, 316, 1646, 842, 382, 484, 860, 7165, 1412, 13367, 290, 20739, 5671, 2346, 17095, 326, 1261, 290, 20739, 2226, 1268, 481, 1682, 316, 1520, 3239, 484, 7163, 6977, 1092, 309, 57369, 25, 11452, 18573, 2236, 480, 802, 261, 1051, 170373, 121706, 23552, 1078, 136719, 326, 70355, 13, 1225, 170331, 261, 5533, 4525, 484, 49012, 5498, 326, 23146, 8525, 271, 220, 17, 13, 392, 48047, 22550, 853, 495, 54587, 83614, 24058, 13, 1225, 8740, 1520, 8512, 316, 4901, 27079, 3543, 538, 7163, 625, 15302, 316, 621, 1412, 480, 6948, 316, 717, 480, 13, 1843, 481, 4128, 1682, 316, 4561, 290, 20425, 11, 1815, 8232, 6675, 591, 290, 16414, 13, 2514, 119602, 290, 1534, 11, 889, 625, 290, 2273, 11, 382, 316, 16218, 67981, 1092, 309, 57369, 25, 11452, 18573, 2236, 480, 5297, 261, 45307, 121706, 24058, 483, 5533, 3814, 13, 1225, 32195, 117852, 10519, 402, 177770, 42506, 483, 10370, 11, 4137, 480, 13127, 472, 261, 77670, 4022, 558, 1944, 271, 220, 18, 13, 392, 976, 19745, 481, 553, 3778, 395, 382, 306, 290, 1101, 481, 553, 49354, 1092, 269, 57369, 25, 11452, 18573, 2236, 480, 802, 82463, 11, 38586, 11, 326, 1051, 170373, 13, 1225, 6008, 5364, 121706, 10519, 1078, 35389, 326, 6942, 2935, 39241, 5018, 3814, 364, 271, 220, 19, 13, 392, 17886, 22295, 382, 290, 2140, 316, 3505, 13, 75835, 1665, 40108, 860, 7165, 1412, 1023, 3195, 656, 24667, 1043, 738, 316, 31827, 1419, 33999, 11, 16928, 503, 11747, 14396, 309, 57369, 25, 220, 11452, 18573, 2236, 480, 21849, 261, 29983, 121706, 30540, 1078, 70355, 326, 39984, 13, 1225, 382, 1051, 170373, 326, 92977, 22158, 5200, 558, 1944, 271, 220, 20, 13, 392, 3538, 113811, 12471, 1327, 1078, 3992, 56470, 9607, 503, 7989, 7281, 6675, 395, 34257, 37005, 26, 4275, 290, 23273, 2870, 634, 23749, 326, 13559, 11, 1919, 1753, 3291, 11, 20434, 3736, 481, 2304, 4044, 2322, 9995, 78034, 869, 6047, 316, 1101, 402, 634, 11366, 16419, 10656, 25920, 2322, 3697, 4795, 1511, 18965, 3832, 8266, 22021, 11, 21087, 6451, 1511, 37556, 326, 38736, 1511, 537, 52038, 562, 9386, 1092, 309, 57369, 25, 11452, 18573, 2236, 480, 802, 261, 3149, 121706, 26368, 484, 25427, 35389, 306, 261, 5364, 11, 117852, 11, 326, 74723, 2006, 13, 1225, 853, 261, 5532, 10526, 11, 10006, 11, 326, 1268, 11, 4137, 480, 13127, 472, 261, 5533, 4022, 558, 1944, 12253, 553, 220, 20, 3120, 134528, 15652, 328, 4564, 82, 484, 2226, 625, 11452, 1985, 326, 484, 481, 2804, 3779, 5093, 483, 15564, 56197, 349, 1953, 1279, 568, 98809, 919, 5933, 115157, 5933, 8, 10669, 538, 1023, 7680, 121706, 540, 1577, 51734, 11, 1023, 2804, 413, 52015, 13, 35307, 290, 35562, 382, 261, 350, 42743, 8, 484, 16644, 4436, 290, 2201, 2226, 625, 32123, 13, 52191, 25, 2804, 12140, 4436, 1023, 7111, 316, 32123, 25, 1879, 35562, 553, 4784, 60592, 11, 21081, 11, 29719, 402, 6217, 3814, 11, 503, 3101, 69471, 316, 3182, 402, 1043, 2316, 472, 261, 121706, 4022, 13, 81913, 1373, 1481, 108715, 503, 185311, 477, 261, 19715, 2236, 1023, 11728, 261, 5364, 10526, 11, 1268, 11, 503, 261, 45307, 9841, 558, 271, 30885, 7413, 271, 220, 16, 13, 392, 27182, 665, 413, 15500, 10069, 1, 793, 530, 57369, 25, 52015, 2236, 4037, 51855, 2201, 2236, 38926, 21081, 11, 71357, 24058, 503, 117852, 10519, 11, 625, 28183, 4951, 395, 261, 4022, 3823, 364, 271, 220, 17, 13, 392, 3436, 10954, 4436, 481, 1757, 3357, 6700, 1, 793, 309, 57369, 25, 52015, 2236, 178374, 402, 12486, 3814, 350, 13347, 673, 20613, 2254, 936, 60592, 538, 42329, 364, 271, 220, 18, 13, 392, 10157, 581, 20613, 2254, 11, 3505, 382, 3378, 1, 793, 309, 57369, 25, 52015, 2236, 95471, 6217, 14266, 11, 6284, 3182, 11238, 11, 69471, 326, 95984, 364, 271, 220, 19, 13, 392, 40, 3508, 316, 9349, 481, 1602, 13, 357, 3508, 316, 5666, 481, 591, 2966, 484, 3479, 3692, 793, 309, 57369, 25, 52015, 2236, 29567, 295, 9710, 11, 71357, 121706, 9660, 503, 29983, 10915, 11, 59698, 9841, 558, 3564, 271, 220, 20, 13, 392, 7943, 357, 2059, 11, 540, 495, 2438, 11, 357, 2411, 9790, 1339, 4113, 842, 1299, 21528, 1647, 1, 793, 309, 57369, 25, 52015, 2236, 19086, 84573, 1962, 2935, 261, 121706, 23552, 11, 71357, 1051, 170373, 24058, 13, 200020, 200021, 9719, 5296, 382, 316, 88495, 11452, 13700, 99117, 1953, 4564, 82, 827, 15564, 1373, 538, 1062, 382, 2491, 306, 290, 23958, 558, 309, 7306, 382, 290, 23958, 481, 2804, 30532, 734, 58, 56156, 1604, 1592, 58, 46713, 13, 3062, 82, 533, 220, 47555, 13, 7767, 82, 60, 70131, 31211, 5306, 480, 1869, 7264, 8881, 558, 58, 47555, 13, 7479, 82, 533, 220, 47292, 13, 1130, 82, 60, 1608, 553, 625, 9113, 656, 634, 198, 58, 47292, 13, 1723, 82, 533, 220, 44136, 13, 1311, 82, 60, 4241, 29348, 26, 1023, 553, 26624, 20636, 198, 58, 44136, 13, 1860, 82, 533, 220, 45587, 13, 3384, 82, 60, 484, 13708, 481, 395, 198, 58, 45587, 13, 8017, 82, 533, 220, 45498, 13, 2270, 82, 60, 10740, 94496, 114121, 585, 6675, 11, 16601, 198, 58, 45498, 13, 3255, 82, 533, 220, 49505, 13, 4129, 82, 60, 5151, 290, 125040, 11, 326, 9334, 484, 198, 58, 37779, 13, 2922, 82, 533, 220, 46240, 13, 7767, 82, 60, 5954, 1511, 9278, 261, 5277, 14255, 198, 58, 46240, 13, 7479, 82, 533, 220, 39478, 13, 4129, 82, 60, 483, 9676, 326, 37556, 198, 58, 44712, 13, 2922, 82, 533, 220, 45157, 13, 2843, 82, 60, 889, 357, 665, 17161, 306, 11, 306, 11, 306, 10326, 480, 1954, 11, 1299, 5477, 6752, 12183, 656, 290, 2840, 484, 1023, 1481, 3810, 484, 198, 58, 45157, 13, 5080, 82, 533, 220, 45458, 13, 2270, 82, 60, 44257, 1495, 316, 717, 12787, 2935, 4811, 3432, 13, 16835, 7698, 4652, 290, 2684, 23552, 558, 58, 45458, 13, 3255, 82, 533, 220, 47829, 13, 3173, 82, 60, 1608, 553, 625, 9113, 198, 58, 47829, 13, 1422, 82, 533, 220, 43749, 13, 2220, 82, 60, 656, 634, 20020, 26, 481, 198, 58, 43749, 13, 9876, 82, 533, 220, 47718, 13, 1422, 82, 60, 13174, 1373, 14166, 3580, 412, 58, 47718, 13, 2092, 82, 533, 220, 45912, 13, 4689, 82, 60, 1520, 21279, 19523, 11, 326, 198, 58, 45912, 13, 6283, 82, 533, 220, 42916, 13, 2290, 82, 60, 13709, 261, 4869, 328, 198, 58, 42916, 13, 994, 82, 533, 220, 47574, 13, 8876, 82, 60, 70023, 326, 70355, 198, 58, 47574, 13, 9912, 82, 533, 220, 47002, 13, 3519, 82, 60, 40057, 483, 634, 1577, 5506, 656, 2966, 316, 290, 3461, 306, 290, 6496, 4895, 503, 23808, 316, 8879, 47525, 578, 1136, 198, 58, 47002, 13, 4635, 82, 533, 220, 41828, 13, 3255, 82, 60, 1958, 357, 679, 290, 25522, 19148, 842, 326, 5477, 4279, 4113, 722, 328, 290, 5123, 484, 5477, 61314, 558, 58, 41828, 13, 3796, 82, 533, 220, 47262, 13, 2843, 82, 60, 1958, 357, 11, 860, 11, 4128, 717, 668, 8201, 558, 58, 56156, 1268, 60, 200020, 200019, 108118, 25, 623, 2201, 993, 164014, 268, 21860, 52304, 326, 261, 14567, 24058, 402, 7524, 20636, 483, 1920, 2823, 121706, 20673, 1078, 114793, 4241, 29348, 326, 20020, 11, 41762, 43204, 326, 3832, 12870, 13, 27067, 402, 484, 9841, 11, 1920, 51855, 121706, 35562, 1504, 2491, 11, 1118, 32123, 5192, 316, 1043, 28085, 328, 3832, 8266, 11, 70355, 11, 75598, 11, 53814, 12847, 11, 326, 70023, 1072, 13525, 26, 1273, 7881, 1504, 52015, 472, 1023, 553, 21081, 11, 43204, 11, 503, 84573, 3556, 2935, 74723, 1432, 26, 290, 10576, 121706, 35562, 553, 5533, 11, 1051, 170373, 11, 326, 621, 625, 11728, 3814, 1261, 42329, 13, 793, 27, 3056, 523, 10817, 56197, 349, 1953, 1279, 13414, 184023, 47555, 13, 7479, 82, 533, 220, 47292, 13, 1130, 82, 60, 1608, 553, 625, 9113, 656, 634, 723, 47292, 13, 1723, 82, 533, 220, 44136, 13, 1311, 82, 60, 4241, 29348, 26, 1023, 553, 26624, 20636, 723, 44136, 13, 1860, 82, 533, 220, 45587, 13, 3384, 82, 60, 484, 13708, 481, 395, 723, 45587, 13, 8017, 82, 533, 220, 45498, 13, 2270, 82, 60, 10740, 94496, 114121, 585, 6675, 11, 16601, 723, 45498, 13, 3255, 82, 533, 220, 49505, 13, 4129, 82, 60, 5151, 290, 125040, 11, 326, 9334, 484, 723, 37779, 13, 2922, 82, 533, 220, 46240, 13, 7767, 82, 60, 5954, 1511, 9278, 261, 5277, 14255, 723, 46240, 13, 7479, 82, 533, 220, 39478, 13, 4129, 82, 60, 483, 9676, 326, 37556, 672, 2201, 5933, 115157, 5933, 446, 10817, 56197, 349, 1953, 1279, 13414, 184023, 45458, 13, 3255, 82, 533, 220, 47829, 13, 3173, 82, 60, 1608, 553, 625, 9113, 723, 47829, 13, 1422, 82, 533, 220, 43749, 13, 2220, 82, 60, 656, 634, 20020, 26, 481, 723, 43749, 13, 9876, 82, 533, 220, 47718, 13, 1422, 82, 60, 13174, 1373, 14166, 3580, 11, 723, 47718, 13, 2092, 82, 533, 220, 45912, 13, 4689, 82, 60, 1520, 21279, 19523, 11, 326, 723, 45912, 13, 6283, 82, 533, 220, 42916, 13, 2290, 82, 60, 13709, 261, 4869, 328, 723, 42916, 13, 994, 82, 533, 220, 47574, 13, 8876, 82, 60, 70023, 326, 70355, 672, 2201, 5933, 115157, 5933, 8, 17196, 65702, 568, 321, 4167, 1896, 808, 3056, 29, 200020, 200020, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029]
[2025-08-20 18:29:38] LABELS: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 108118, 25, 623, 2201, 993, 164014, 268, 21860, 52304, 326, 261, 14567, 24058, 402, 7524, 20636, 483, 1920, 2823, 121706, 20673, 1078, 114793, 4241, 29348, 326, 20020, 11, 41762, 43204, 326, 3832, 12870, 13, 27067, 402, 484, 9841, 11, 1920, 51855, 121706, 35562, 1504, 2491, 11, 1118, 32123, 5192, 316, 1043, 28085, 328, 3832, 8266, 11, 70355, 11, 75598, 11, 53814, 12847, 11, 326, 70023, 1072, 13525, 26, 1273, 7881, 1504, 52015, 472, 1023, 553, 21081, 11, 43204, 11, 503, 84573, 3556, 2935, 74723, 1432, 26, 290, 10576, 121706, 35562, 553, 5533, 11, 1051, 170373, 11, 326, 621, 625, 11728, 3814, 1261, 42329, 13, 793, 27, 3056, 523, 10817, 56197, 349, 1953, 1279, 13414, 184023, 47555, 13, 7479, 82, 533, 220, 47292, 13, 1130, 82, 60, 1608, 553, 625, 9113, 656, 634, 723, 47292, 13, 1723, 82, 533, 220, 44136, 13, 1311, 82, 60, 4241, 29348, 26, 1023, 553, 26624, 20636, 723, 44136, 13, 1860, 82, 533, 220, 45587, 13, 3384, 82, 60, 484, 13708, 481, 395, 723, 45587, 13, 8017, 82, 533, 220, 45498, 13, 2270, 82, 60, 10740, 94496, 114121, 585, 6675, 11, 16601, 723, 45498, 13, 3255, 82, 533, 220, 49505, 13, 4129, 82, 60, 5151, 290, 125040, 11, 326, 9334, 484, 723, 37779, 13, 2922, 82, 533, 220, 46240, 13, 7767, 82, 60, 5954, 1511, 9278, 261, 5277, 14255, 723, 46240, 13, 7479, 82, 533, 220, 39478, 13, 4129, 82, 60, 483, 9676, 326, 37556, 672, 2201, 5933, 115157, 5933, 446, 10817, 56197, 349, 1953, 1279, 13414, 184023, 45458, 13, 3255, 82, 533, 220, 47829, 13, 3173, 82, 60, 1608, 553, 625, 9113, 723, 47829, 13, 1422, 82, 533, 220, 43749, 13, 2220, 82, 60, 656, 634, 20020, 26, 481, 723, 43749, 13, 9876, 82, 533, 220, 47718, 13, 1422, 82, 60, 13174, 1373, 14166, 3580, 11, 723, 47718, 13, 2092, 82, 533, 220, 45912, 13, 4689, 82, 60, 1520, 21279, 19523, 11, 326, 723, 45912, 13, 6283, 82, 533, 220, 42916, 13, 2290, 82, 60, 13709, 261, 4869, 328, 723, 42916, 13, 994, 82, 533, 220, 47574, 13, 8876, 82, 60, 70023, 326, 70355, 672, 2201, 5933, 115157, 5933, 8, 17196, 65702, 568, 321, 4167, 1896, 808, 3056, 29, 200020, 200020, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029, 200029]
[2025-08-20 18:29:38] Masked tokens: 3111
[2025-08-20 18:29:38] Her er teksten som blir kalkulert loss på (LOSS TEXT): Thought: The text intersperses casual remarks and a brief insight on learning lessons with two direct motivational statements about overcoming past mistakes and circumstances, amid promotional and personal notes. Based on that intent, two qualifying motivational texts were found, which qualify due to their themes of personal growth, resilience, forgiveness, mindset shift, and triumph over challenges; other parts were excluded as they are generic, promotional, or anecdotal without inspirational value; the saved motivational texts are complete, self-contained, and do not lack context when isolated. 
<code>
SaveMotivationalText(text="[719.89s - 724.30s] You are not defined by your [724.40s - 728.13s] past mistakes; they are merely lessons [728.23s - 732.75s] that prepare you for [732.85s - 736.29s] greater victories Forgive yourself, rise [736.39s - 739.95s] above the regrets, and channel that [740.05s - 744.79s] energy into creating a future filled [744.89s - 748.95s] with purpose and achievement", text_file=text_file)
SaveMotivationalText(text="[756.39s - 761.07s] You are not defined [761.17s - 765.64s] by your circumstances; you [765.74s - 769.17s] define them Take control, [769.27s - 773.42s] make bold moves, and [773.52s - 778.01s] craft a story of [778.11s - 782.63s] triumph and resilience", text_file=text_file)final_answer("im done")
</code><|end|><|end|><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜><｜PAD▁TOKEN｜>
[2025-08-20 18:29:38] --------------------------------------------------------------------------------
[2025-08-20 18:29:44] 
----------------Starting Training-------------

[2025-08-21 17:54:21] 
----------------Loading/initalizing MODEL-------------

[2025-08-21 17:54:33] Model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)
          (qkv_proj): Linear4bit(in_features=3072, out_features=5120, bias=False)
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)
[2025-08-21 17:54:33] clearing cache after model loading...
[2025-08-21 17:54:33] 
----------------Loading tokenizer-------------

[2025-08-21 17:54:33] tokenizer length: 200030
[2025-08-21 17:54:33] tokenizer.pad token was None changed it to : <｜PAD▁TOKEN｜>
[2025-08-21 17:54:33] Vocab length BEFORE training: 200030
[2025-08-21 17:54:33] Tokenizer.pad_token already exist: <｜PAD▁TOKEN｜>
[2025-08-21 17:54:33] clearing cache after tokenizer...
[2025-08-21 17:54:33] clearing cache after prepare model for kbit training...
[2025-08-21 17:54:34] 🟢 Trainable parameters: ['base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight']
[2025-08-21 17:54:34] 
⚪️ Frozen parameters: ['base_model.model.model.embed_tokens.weight', 'base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.0.mlp.gate_up_proj.weight', 'base_model.model.model.layers.0.mlp.down_proj.base_layer.weight']
[2025-08-21 17:54:34] Perft_config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='C:\\Users\\didri\\Desktop\\LLM-models\\LLM-Models\\microsoft\\unsloth\\Phi-4-mini-Instruct-finetuned-Motivational-text', revision=None, inference_mode=False, r=16, target_modules={'o_proj', 'down_proj', 'qkv_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.03, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None) 
 
[2025-08-21 17:54:34] Model is prepared for kbit training now...
 model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (qkv_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=5120, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=5120, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=8192, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)

[2025-08-21 17:54:34] model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (qkv_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=5120, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=5120, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=8192, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)

[2025-08-21 17:54:34] 
----------------Dataset Initizalation-------------

[2025-08-21 17:55:06] 
----------------Loading/initalizing MODEL-------------

[2025-08-21 17:55:12] Model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)
          (qkv_proj): Linear4bit(in_features=3072, out_features=5120, bias=False)
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)
[2025-08-21 17:55:12] clearing cache after model loading...
[2025-08-21 17:55:12] 
----------------Loading tokenizer-------------

[2025-08-21 17:55:12] tokenizer length: 200030
[2025-08-21 17:55:12] tokenizer.pad token was None changed it to : <｜PAD▁TOKEN｜>
[2025-08-21 17:55:12] Vocab length BEFORE training: 200030
[2025-08-21 17:55:12] Tokenizer.pad_token already exist: <｜PAD▁TOKEN｜>
[2025-08-21 17:55:12] clearing cache after tokenizer...
[2025-08-21 17:55:12] clearing cache after prepare model for kbit training...
[2025-08-21 17:55:13] 🟢 Trainable parameters: ['base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight']
[2025-08-21 17:55:13] 
⚪️ Frozen parameters: ['base_model.model.model.embed_tokens.weight', 'base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.0.mlp.gate_up_proj.weight', 'base_model.model.model.layers.0.mlp.down_proj.base_layer.weight']
[2025-08-21 17:55:13] Perft_config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='C:\\Users\\didri\\Desktop\\LLM-models\\LLM-Models\\microsoft\\unsloth\\Phi-4-mini-Instruct-finetuned-Motivational-text', revision=None, inference_mode=False, r=16, target_modules={'qkv_proj', 'down_proj', 'o_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.03, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None) 
 
[2025-08-21 17:55:13] Model is prepared for kbit training now...
 model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (qkv_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=5120, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=5120, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=8192, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)

[2025-08-21 17:55:13] model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (qkv_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=5120, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=5120, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=8192, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)

[2025-08-21 17:55:13] 
----------------Dataset Initizalation-------------

[2025-08-21 17:55:13] clearing cache after loading dataset
[2025-08-21 17:55:43] clearing cache after validation test
[2025-08-21 17:55:43] ----------------Starting Training now----------------
[2025-08-21 17:59:43] 
----------------Loading/initalizing MODEL-------------

[2025-08-21 17:59:49] Model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)
          (qkv_proj): Linear4bit(in_features=3072, out_features=5120, bias=False)
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)
[2025-08-21 17:59:49] clearing cache after model loading...
[2025-08-21 17:59:49] 
----------------Loading tokenizer-------------

[2025-08-21 17:59:49] tokenizer length: 200030
[2025-08-21 17:59:49] tokenizer.pad token was None changed it to : <｜PAD▁TOKEN｜>
[2025-08-21 17:59:49] Vocab length BEFORE training: 200030
[2025-08-21 17:59:49] Tokenizer.pad_token already exist: <｜PAD▁TOKEN｜>
[2025-08-21 17:59:49] clearing cache after tokenizer...
[2025-08-21 17:59:49] clearing cache after prepare model for kbit training...
[2025-08-21 17:59:50] 🟢 Trainable parameters: ['base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight']
[2025-08-21 17:59:50] 
⚪️ Frozen parameters: ['base_model.model.model.embed_tokens.weight', 'base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.0.mlp.gate_up_proj.weight', 'base_model.model.model.layers.0.mlp.down_proj.base_layer.weight']
[2025-08-21 17:59:50] Perft_config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='C:\\Users\\didri\\Desktop\\LLM-models\\LLM-Models\\microsoft\\unsloth\\Phi-4-mini-Instruct-finetuned-Motivational-text', revision=None, inference_mode=False, r=16, target_modules={'o_proj', 'qkv_proj', 'down_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.03, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None) 
 
[2025-08-21 17:59:50] Model is prepared for kbit training now...
 model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (qkv_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=5120, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=5120, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=8192, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)

[2025-08-21 17:59:50] model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (qkv_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=5120, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=5120, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=8192, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)

[2025-08-21 17:59:50] 
----------------Dataset Initizalation-------------

[2025-08-21 17:59:50] clearing cache after loading dataset
[2025-08-21 18:00:05] clearing cache after validation test
[2025-08-21 18:00:05] ----------------Starting Training now----------------
[2025-08-21 18:05:21] 
----------------Loading/initalizing MODEL-------------

[2025-08-21 18:05:28] Model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)
          (qkv_proj): Linear4bit(in_features=3072, out_features=5120, bias=False)
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)
[2025-08-21 18:05:28] clearing cache after model loading...
[2025-08-21 18:05:29] 
----------------Loading tokenizer-------------

[2025-08-21 18:05:29] tokenizer length: 200030
[2025-08-21 18:05:29] tokenizer.pad token was None changed it to : <｜PAD▁TOKEN｜>
[2025-08-21 18:05:29] Vocab length BEFORE training: 200030
[2025-08-21 18:05:29] Tokenizer.pad_token already exist: <｜PAD▁TOKEN｜>
[2025-08-21 18:05:29] clearing cache after tokenizer...
[2025-08-21 18:05:29] clearing cache after prepare model for kbit training...
[2025-08-21 18:05:30] 🟢 Trainable parameters: ['base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight']
[2025-08-21 18:05:30] 
⚪️ Frozen parameters: ['base_model.model.model.embed_tokens.weight', 'base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.0.mlp.gate_up_proj.weight', 'base_model.model.model.layers.0.mlp.down_proj.base_layer.weight']
[2025-08-21 18:05:30] Perft_config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='C:\\Users\\didri\\Desktop\\LLM-models\\LLM-Models\\microsoft\\unsloth\\Phi-4-mini-Instruct-finetuned-Motivational-text', revision=None, inference_mode=False, r=16, target_modules={'qkv_proj', 'o_proj', 'down_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.03, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None) 
 
[2025-08-21 18:05:30] Model is prepared for kbit training now...
 model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (qkv_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=5120, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=5120, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=8192, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)

[2025-08-21 18:05:30] model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (qkv_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=5120, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=5120, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=8192, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)

[2025-08-21 18:05:30] 
----------------Dataset Initizalation-------------

[2025-08-21 18:05:30] clearing cache after loading dataset
[2025-08-21 18:06:03] clearing cache after validation test
[2025-08-21 18:06:03] ----------------Starting Training now----------------
[2025-08-21 18:24:32] 
----------------Loading/initalizing MODEL-------------

[2025-08-21 18:24:38] Model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)
          (qkv_proj): Linear4bit(in_features=3072, out_features=5120, bias=False)
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)
[2025-08-21 18:24:38] clearing cache after model loading...
[2025-08-21 18:24:38] 
----------------Loading tokenizer-------------

[2025-08-21 18:24:38] tokenizer length: 200030
[2025-08-21 18:24:38] tokenizer.pad token was None changed it to : <｜PAD▁TOKEN｜>
[2025-08-21 18:24:38] Vocab length BEFORE training: 200030
[2025-08-21 18:24:38] Tokenizer.pad_token already exist: <｜PAD▁TOKEN｜>
[2025-08-21 18:24:38] clearing cache after tokenizer...
[2025-08-21 18:24:38] clearing cache after prepare model for kbit training...
[2025-08-21 18:24:39] 🟢 Trainable parameters: ['base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight']
[2025-08-21 18:24:39] 
⚪️ Frozen parameters: ['base_model.model.model.embed_tokens.weight', 'base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.0.mlp.gate_up_proj.weight', 'base_model.model.model.layers.0.mlp.down_proj.base_layer.weight']
[2025-08-21 18:24:39] Perft_config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='C:\\Users\\didri\\Desktop\\LLM-models\\LLM-Models\\microsoft\\unsloth\\Phi-4-mini-Instruct-finetuned-Motivational-text', revision=None, inference_mode=False, r=16, target_modules={'o_proj', 'qkv_proj', 'down_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.03, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None) 
 
[2025-08-21 18:24:39] Model is prepared for kbit training now...
 model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (qkv_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=5120, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=5120, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=8192, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)

[2025-08-21 18:24:39] model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (qkv_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=5120, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=5120, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=8192, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)

[2025-08-21 18:24:39] 
----------------Dataset Initizalation-------------

[2025-08-21 18:24:39] clearing cache after loading dataset
[2025-08-21 18:25:16] clearing cache after validation test
[2025-08-21 18:25:16] ----------------Starting Training now----------------
[2025-08-21 18:26:47] 
----------------Loading/initalizing MODEL-------------

[2025-08-21 18:26:58] Model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)
          (qkv_proj): Linear4bit(in_features=3072, out_features=5120, bias=False)
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)
[2025-08-21 18:26:58] clearing cache after model loading...
[2025-08-21 18:26:58] 
----------------Loading tokenizer-------------

[2025-08-21 18:26:59] tokenizer length: 200030
[2025-08-21 18:26:59] tokenizer.pad token was None changed it to : <｜PAD▁TOKEN｜>
[2025-08-21 18:26:59] Vocab length BEFORE training: 200030
[2025-08-21 18:26:59] Tokenizer.pad_token already exist: <｜PAD▁TOKEN｜>
[2025-08-21 18:26:59] clearing cache after tokenizer...
[2025-08-21 18:26:59] clearing cache after prepare model for kbit training...
[2025-08-21 18:26:59] 🟢 Trainable parameters: ['base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight']
[2025-08-21 18:26:59] 
⚪️ Frozen parameters: ['base_model.model.model.embed_tokens.weight', 'base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.0.mlp.gate_up_proj.weight', 'base_model.model.model.layers.0.mlp.down_proj.base_layer.weight']
[2025-08-21 18:26:59] Perft_config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='C:\\Users\\didri\\Desktop\\LLM-models\\LLM-Models\\microsoft\\unsloth\\Phi-4-mini-Instruct-finetuned-Motivational-text', revision=None, inference_mode=False, r=16, target_modules={'down_proj', 'qkv_proj', 'o_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.03, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None) 
 
[2025-08-21 18:26:59] Model is prepared for kbit training now...
 model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (qkv_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=5120, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=5120, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=8192, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)

[2025-08-21 18:26:59] model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (qkv_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=5120, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=5120, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=8192, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)

[2025-08-21 18:26:59] 
----------------Dataset Initizalation-------------

[2025-08-21 18:27:00] clearing cache after loading dataset
[2025-08-21 18:27:01] clearing cache after validation test
[2025-08-21 18:27:01] ----------------Starting Training now----------------
[2025-08-21 18:40:11] 
----------------Loading/initalizing MODEL-------------

[2025-08-21 18:40:22] Model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)
          (qkv_proj): Linear4bit(in_features=3072, out_features=5120, bias=False)
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)
[2025-08-21 18:40:22] clearing cache after model loading...
[2025-08-21 18:40:23] 
----------------Loading tokenizer-------------

[2025-08-21 18:40:23] tokenizer length: 200030
[2025-08-21 18:40:23] tokenizer.pad token was None changed it to : <｜PAD▁TOKEN｜>
[2025-08-21 18:40:23] Vocab length BEFORE training: 200030
[2025-08-21 18:40:23] Tokenizer.pad_token already exist: <｜PAD▁TOKEN｜>
[2025-08-21 18:40:23] clearing cache after tokenizer...
[2025-08-21 18:40:23] clearing cache after prepare model for kbit training...
[2025-08-21 18:40:24] 🟢 Trainable parameters: ['base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight']
[2025-08-21 18:40:24] 
⚪️ Frozen parameters: ['base_model.model.model.embed_tokens.weight', 'base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.0.mlp.gate_up_proj.weight', 'base_model.model.model.layers.0.mlp.down_proj.base_layer.weight']
[2025-08-21 18:40:24] Perft_config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='C:\\Users\\didri\\Desktop\\LLM-models\\LLM-Models\\microsoft\\unsloth\\Phi-4-mini-Instruct-finetuned-Motivational-text', revision=None, inference_mode=False, r=16, target_modules={'qkv_proj', 'down_proj', 'o_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.03, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None) 
 
[2025-08-21 18:40:24] Model is prepared for kbit training now...
 model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (qkv_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=5120, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=5120, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=8192, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)

[2025-08-21 18:40:24] model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (qkv_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=5120, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=5120, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=8192, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)

[2025-08-21 18:40:24] 
----------------Dataset Initizalation-------------

[2025-08-21 18:40:24] clearing cache after loading dataset
[2025-08-21 18:40:58] clearing cache after validation test
[2025-08-21 18:40:58] ----------------Starting Training now----------------
[2025-08-21 19:00:17] 
----------------Loading/initalizing MODEL-------------

[2025-08-21 19:00:26] Model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)
          (qkv_proj): Linear4bit(in_features=3072, out_features=5120, bias=False)
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)
[2025-08-21 19:00:26] clearing cache after model loading...
[2025-08-21 19:00:26] 
----------------Loading tokenizer-------------

[2025-08-21 19:00:27] tokenizer length: 200030
[2025-08-21 19:00:27] tokenizer.pad token was None changed it to : <｜PAD▁TOKEN｜>
[2025-08-21 19:00:27] Vocab length BEFORE training: 200030
[2025-08-21 19:00:27] Tokenizer.pad_token already exist: <｜PAD▁TOKEN｜>
[2025-08-21 19:00:27] clearing cache after tokenizer...
[2025-08-21 19:00:27] clearing cache after prepare model for kbit training...
[2025-08-21 19:00:27] 🟢 Trainable parameters: ['base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight']
[2025-08-21 19:00:27] 
⚪️ Frozen parameters: ['base_model.model.model.embed_tokens.weight', 'base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.0.mlp.gate_up_proj.weight', 'base_model.model.model.layers.0.mlp.down_proj.base_layer.weight']
[2025-08-21 19:00:27] Perft_config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='C:\\Users\\didri\\Desktop\\LLM-models\\LLM-Models\\microsoft\\unsloth\\Phi-4-mini-Instruct-finetuned-Motivational-text', revision=None, inference_mode=False, r=16, target_modules={'qkv_proj', 'down_proj', 'o_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.03, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None) 
 
[2025-08-21 19:00:27] Model is prepared for kbit training now...
 model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (qkv_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=5120, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=5120, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=8192, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)

[2025-08-21 19:00:27] model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (qkv_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=5120, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=5120, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=8192, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)

[2025-08-21 19:00:27] 
----------------Dataset Initizalation-------------

[2025-08-21 19:00:28] clearing cache after loading dataset
[2025-08-21 19:01:02] clearing cache after validation test
[2025-08-21 19:01:02] ----------------Starting Training now----------------
[2025-08-21 19:26:15] clearing cache after training complete
[2025-08-21 19:26:16] clearing cache after evaluation complete
[2025-08-21 19:26:16] Successfully done finetuning!
[2025-08-21 19:26:16] --------------Running a manual test--------------------


[2025-08-21 19:26:16] SUCCESS :D
[2025-08-21 21:15:29] 
----------------Loading/initalizing MODEL-------------

[2025-08-21 21:15:35] Model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)
          (qkv_proj): Linear4bit(in_features=3072, out_features=5120, bias=False)
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)
[2025-08-21 21:15:35] clearing cache after model loading...
[2025-08-21 21:15:35] 
----------------Loading tokenizer-------------

[2025-08-21 21:15:36] tokenizer length: 200030
[2025-08-21 21:15:36] tokenizer.pad token was None changed it to : <｜PAD▁TOKEN｜>
[2025-08-21 21:15:36] Vocab length BEFORE training: 200030
[2025-08-21 21:15:36] Tokenizer.pad_token already exist: <｜PAD▁TOKEN｜>
[2025-08-21 21:15:36] clearing cache after tokenizer...
[2025-08-21 21:15:36] clearing cache after prepare model for kbit training...
[2025-08-21 21:15:36] 🟢 Trainable parameters: ['base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight']
[2025-08-21 21:15:36] 
⚪️ Frozen parameters: ['base_model.model.model.embed_tokens.weight', 'base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.0.mlp.gate_up_proj.weight', 'base_model.model.model.layers.0.mlp.down_proj.base_layer.weight']
[2025-08-21 21:15:36] Perft_config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='C:\\Users\\didri\\Desktop\\LLM-models\\LLM-Models\\microsoft\\unsloth\\Phi-4-mini-Instruct-finetuned-Motivational-text', revision=None, inference_mode=False, r=16, target_modules={'qkv_proj', 'down_proj', 'o_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.03, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None) 
 
[2025-08-21 21:15:36] Model is prepared for kbit training now...
 model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (qkv_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=5120, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=5120, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=8192, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)

[2025-08-21 21:15:36] model: Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(200064, 3072, padding_idx=200029)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (qkv_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=3072, out_features=5120, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=3072, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=5120, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)
          (down_proj): lora.Linear4bit(
            (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.03, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=8192, out_features=16, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=16, out_features=3072, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)
)

[2025-08-21 21:15:36] 
----------------Dataset Initizalation-------------

[2025-08-21 21:15:37] clearing cache after loading dataset
[2025-08-21 21:16:11] clearing cache after validation test
[2025-08-21 21:16:11] ----------------Starting Training now----------------
[2025-08-21 21:40:36] clearing cache after training complete
[2025-08-21 21:40:37] clearing cache after evaluation complete
[2025-08-21 21:40:37] Successfully done finetuning!
[2025-08-21 21:40:37] --------------Running a manual test--------------------


[2025-08-21 21:40:37] SUCCESS :D
