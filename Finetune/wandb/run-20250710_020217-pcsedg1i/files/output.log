100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:35<00:00, 17.04s/it][34m[1mwandb[0m: [33mWARNING[0m The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.
[----------------TRAINING METRICS--------------][2025-07-10 02:02:42] TRAIN | Step: 1 | Epoch: 0.02 | Samples: 1
🔹 Loss: 0.3130 | Perplexity: 1.3675215310276052
🔹 LR: 0.0002
🔹 Grad Norm: 4.286715030670166
🔹 #Tokens: 1886.0
🔹 Token Acc: 0.91847825050354

🚀 Starter opp!
Første loss – bruker som referanse fremover.
------------------------------------------------------------



{'loss': 0.313, 'grad_norm': 4.286715030670166, 'learning_rate': 0.0002, 'num_tokens': 1886.0, 'mean_token_accuracy': 0.91847825050354, 'epoch': 0.02}
[----------------TRAINING METRICS--------------][2025-07-10 02:02:55] TRAIN | Step: 2 | Epoch: 0.03 | Samples: 2
🔹 Loss: 0.5013 | Perplexity: 1.6508660021254151
🔹 LR: 0.0001
🔹 Grad Norm: 3.074195623397827
🔹 #Tokens: 3370.0
🔹 Token Acc: 0.9375

⚠️ Loss øker mye! (0.1883)
Mulig overtrening, datastøy eller for høy læringsrate.
------------------------------------------------------------



{'loss': 0.5013, 'grad_norm': 3.074195623397827, 'learning_rate': 0.0001, 'num_tokens': 3370.0, 'mean_token_accuracy': 0.9375, 'epoch': 0.03}
C:\Users\didri\AppData\Local\Programs\Python\Python311\Lib\site-packages\peft\utils\save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:37<00:00, 18.92s/it]
{'train_runtime': 39.9042, 'train_samples_per_second': 0.05, 'train_steps_per_second': 0.05, 'train_loss': 0.4071485996246338, 'epoch': 0.03}
Successfully done finetuning!
🔄 Loading tokenizer from adapter checkpoint...
🔄 Loading base model...
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.08it/s]
Base model vocab size: 151936
🔄 Loading LoRA adapter...
Error during merging: Error(s) in loading state_dict for PeftModelForCausalLM:
	size mismatch for base_model.model.model.embed_tokens.weight: copying a param with shape torch.Size([151680, 2048]) from checkpoint, the shape in current model is torch.Size([151936, 2048]).
	size mismatch for base_model.model.lm_head.weight: copying a param with shape torch.Size([151680, 2048]) from checkpoint, the shape in current model is torch.Size([151936, 2048]).
trying to merge another way
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:16<00:00,  8.45s/it]
Traceback (most recent call last):
  File "c:\Users\didri\Desktop\Full-Agent-Flow_VideoEditing\Finetune\fine_tune_mistrail_copy.py", line 138, in supervised_Finetune
    merge_adapter_checkpoint(base_model_path,adapter_path,output_dir)
  File "c:\Users\didri\Desktop\Full-Agent-Flow_VideoEditing\Finetune\merge_and_unload.py", line 29, in merge_adapter_checkpoint
    peft_model = PeftModel.from_pretrained(base_model, adapter_path)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\didri\AppData\Local\Programs\Python\Python311\Lib\site-packages\peft\peft_model.py", line 541, in from_pretrained
    load_result = model.load_adapter(
                  ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\didri\AppData\Local\Programs\Python\Python311\Lib\site-packages\peft\peft_model.py", line 1276, in load_adapter
    load_result = set_peft_model_state_dict(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\didri\AppData\Local\Programs\Python\Python311\Lib\site-packages\peft\utils\save_and_load.py", line 448, in set_peft_model_state_dict
    load_result = model.load_state_dict(peft_model_state_dict, strict=False)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\didri\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py", line 2593, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM:
	size mismatch for base_model.model.model.embed_tokens.weight: copying a param with shape torch.Size([151680, 2048]) from checkpoint, the shape in current model is torch.Size([151936, 2048]).
	size mismatch for base_model.model.lm_head.weight: copying a param with shape torch.Size([151680, 2048]) from checkpoint, the shape in current model is torch.Size([151936, 2048]).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\didri\Desktop\Full-Agent-Flow_VideoEditing\Finetune\fine_tune_mistrail_copy.py", line 148, in <module>
    supervised_Finetune()
  File "c:\Users\didri\Desktop\Full-Agent-Flow_VideoEditing\Finetune\fine_tune_mistrail_copy.py", line 143, in supervised_Finetune
    merge_adapter_checkpoint_2(adapter_path,output_dir)
  File "c:\Users\didri\Desktop\Full-Agent-Flow_VideoEditing\Finetune\merge_and_unload.py", line 62, in merge_adapter_checkpoint_2
    peft_model = AutoPeftModelForCausalLM.from_pretrained(adapter_path)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\didri\AppData\Local\Programs\Python\Python311\Lib\site-packages\peft\auto.py", line 142, in from_pretrained
    return cls._target_peft_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\didri\AppData\Local\Programs\Python\Python311\Lib\site-packages\peft\peft_model.py", line 541, in from_pretrained
    load_result = model.load_adapter(
                  ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\didri\AppData\Local\Programs\Python\Python311\Lib\site-packages\peft\peft_model.py", line 1276, in load_adapter
    load_result = set_peft_model_state_dict(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\didri\AppData\Local\Programs\Python\Python311\Lib\site-packages\peft\utils\save_and_load.py", line 448, in set_peft_model_state_dict
    load_result = model.load_state_dict(peft_model_state_dict, strict=False)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\didri\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py", line 2593, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM:
	size mismatch for base_model.model.model.embed_tokens.weight: copying a param with shape torch.Size([151680, 2048]) from checkpoint, the shape in current model is torch.Size([151936, 2048]).
	size mismatch for base_model.model.lm_head.weight: copying a param with shape torch.Size([151680, 2048]) from checkpoint, the shape in current model is torch.Size([151936, 2048]).
