from transformers import TrainerCallback, TrainerControl, TrainerState, TrainingArguments, Trainer
from datetime import datetime
from math import exp
class LossAndEvalloggingCallback(TrainerCallback):
    def __init__(self):
        self.prev_train_loss = None
        self.prev_eval_loss = None 
        self.last_logged_train_loss = None 


    def _write_train_log(self, args, state, logs):
        loss = logs.get("loss")
        grad_norm = logs.get("grad_norm", "N/A")
        lr = logs.get("learning_rate", "N/A")
        num_tokens = logs.get("num_tokens", "N/A")
        token_acc = logs.get("mean_token_accuracy", "N/A")
        step = state.global_step
        epoch = state.epoch or 0.0
        samples_seen = args.per_device_train_batch_size * args.gradient_accumulation_steps * step
        perplexity = exp(loss) if loss is not None else "N/A"

    


        token_acc_percent = f"{token_acc * 100:.2f}%" if token_acc is not None else "N/A"

        #Tabell header
        if step == 1 or step % 100 == 0:
            header = (
                f"\n{'Step':>5} | {'Loss':>7} | {'Perplexity':>10} | {'Grad Norm':>10} | {'Token Acc':>10} | {'Kommentar'}\n"
                + "-" * 75
            )
        else:
            header = ""

        row = f"{step:5} | {loss:.4f} | {perplexity:10.4f} | {grad_norm:10.4f} | {token_acc_percent:>9} "

        explanation = f"""
                        ðŸ“˜ Forklaringer:
                        ðŸ”¹ Loss        â€“ MÃ¥l pÃ¥ feil; lavere = bedre.
                        ðŸ”¹ Perplexity  â€“ exp(loss); hvor "forvirret" modellen er (nÃ¦r 1 = bra).
                        ðŸ”¹ Grad Norm   â€“ Hvor kraftige oppdateringer modellen gjÃ¸r (indikator pÃ¥ lÃ¦ring/stabilitet).
                        ðŸ”¹ Token Acc   â€“ Hvor mange tokens som ble korrekt forutsagt (%).
                        ðŸ”¹ Kommentar   â€“ Rask vurdering basert pÃ¥ loss-endring.
                        """
        
        log_msg = f"{header}\n{row}\n{explanation if step % 200 == 0 else ''}"

        log_msg = (
            f"[----------------TRAINING METRICS--------------]"
            f"[{datetime.now():%Y-%m-%d %H:%M:%S}] TRAIN | Step: {step} | Epoch: {epoch:.2f} | Samples: {samples_seen}\n"
            f"ðŸ”¹ Loss: {f'{loss:.4f}' if loss is not None else 'N/A'} | Perplexity: {perplexity}\n"
            f"ðŸ”¹ LR: {lr}\n"
            f"ðŸ”¹ Grad Norm: {grad_norm}\n"
            f"ðŸ”¹ #Tokens: {num_tokens}\n"
            f"ðŸ”¹ Token Acc: {token_acc}\n\n"
            + "-"*60
            + "\n\n\n"
        )
        print(log_msg)
        with open(r"C:\Users\didri\Desktop\Full-Agent-Flow_VideoEditing\Finetune\Dataset_detecting_motivationalquotes_from_chunk\Supervised_dataset\logs\finetuning_loss_logg.txt", "a", encoding="utf-8") as f:
            f.write(log_msg + "\n")

    def on_log(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, logs=None, **kwargs):
        # KjÃ¸r treningsâ€‘logging nÃ¥r loss finnes
        if logs and logs.get("loss") is not None:
            self._write_train_log(args, state, logs)
            self.last_logged_train_loss = logs["loss"]

    def on_evaluate(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, metrics=None, **kwargs):
        if metrics is None:
            return

        eval_loss = metrics.get("eval_loss")
        step = state.global_step
        epoch = state.epoch or 0.0
        eval_perplexity = exp(eval_loss) if eval_loss is not None else "N/A"

   
        if eval_loss is not None:
            delta = eval_loss - self.prev_eval_loss if self.prev_eval_loss is not None else "N/A"
            self.prev_eval_loss = eval_loss

            if delta is None:
                eval_comment = "ðŸš€ FÃ¸rste eval-runde â€“ bruker som referanse."
            elif delta < -0.05:
                eval_comment = "âœ… Eval loss gikk tydelig ned â€“ modellen lÃ¦rer fortsatt godt."
            elif delta < -0.01:
                eval_comment = "ðŸ“‰ Eval loss litt lavere â€“ forbedring."
            elif abs(delta) < 0.005:
                eval_comment = "ðŸ˜ Eval loss stÃ¥r omtrent stille â€“ kanskje metningspunkt?"
            elif delta > 0.05:
                eval_comment = "âš ï¸ Eval loss Ã¸ker mye â€“ mulig overfitting eller dÃ¥rlig data."
            else:
                eval_comment = "ðŸ”„ Eval loss litt opp â€“ fÃ¸lg med pÃ¥ trend og overtrening."
        else:
            eval_comment = "â„¹ï¸ Ingen eval-loss logget."


        header = (
            f"\n{'Step':>5} | {'Eval Loss':>10} | {'Perplexity':>10} | {'Kommentar'}\n"
            + "-" * 60
        )
        row = f"{step:5} | "
        row += f"{eval_loss:.4f}" if eval_loss is not None else "   N/A   "
        row += "    | "
        row += f"{eval_perplexity:10.4f}" if isinstance(eval_perplexity, (float,int)) else "      N/A   "
        row += f" | {eval_comment}"


      
        explanation = f"""
                    ðŸ“˜ Eval Forklaringer:
                    ðŸ”¹ Eval Loss   â€“ MÃ¥l pÃ¥ feil pÃ¥ valideringsdata. Viktig for Ã¥ oppdage overfitting.
                    ðŸ”¹ Perplexity  â€“ exp(eval_loss); nÃ¦r 1 betyr lite usikkerhet.
                    ðŸ”¹ Kommentar   â€“ Tolker endring i eval-loss siden forrige runde.
                    """


        train_loss = self.prev_train_loss
        fit_comment = ""
        if train_loss is not None and eval_loss is not None:
            loss_gap = eval_loss - train_loss
            if loss_gap > 0.4:
                fit_comment = (
                    "\nðŸ§  Overfitting mistenkes: Eval loss mye hÃ¸yere enn train loss."
                    "\nðŸ”§ Tiltak: PrÃ¸v mer regularisering, mindre modell, mer data eller early stopping."
                )
            elif loss_gap < -0.2:
                fit_comment = (
                    "\nðŸ¤” Eval loss lavere enn train loss â€“ kanskje eval-sett er enklere?"
                    "\nðŸ” UndersÃ¸k eval-data for skjevheter eller 'lekkasjer'."
                )
            else:
                fit_comment = "\nâœ… Eval loss og train loss er balanserte â€“ modellen ser ut til Ã¥ generalisere godt."

    
        full_log = (
            f"[----------------EVALUATION METRICS--------------]"
            f"\n[{datetime.now():%Y-%m-%d %H:%M:%S}]  EVAL  | Step: {step} | Epoch: {epoch:.2f}"
            f"\n{header}\n{row}\n{explanation}"
            f"{fit_comment}"
            f"\n{'-'*60}\n"
        )

        print(full_log)
        with open(
            r"C:\Users\didri\Desktop\Full-Agent-Flow_VideoEditing\Finetune\Dataset_detecting_motivationalquotes_from_chunk\Supervised_dataset\logs\finetuning_loss_logg.txt",
            "a",
            encoding="utf-8"
        ) as f:
            f.write(full_log + "\n")
